{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1D Convolutional Layers\n",
    "\n",
    "Convolutional Neural Networks (ConvNets) perform particularly well on computer vision problems due to their ability to operate convolutionally, that is extracting features from local input patches allowing for representation modularity and data efficiency. The same properties that make ConvNets the best choice for computer vision-related problems also make them highly significant to sequence processing. 1D convolution layers are also translation invariant in the sense that because the same input transformation is performed on every patch, a pattern learned at a certain position in a sentence can later be recognized at a different position. Similar to 2D ConvNets, 1D patches can be extracted from an input and output the maximum or average value, a process technically referred to as Max Pooling and Average Pooling respectively, and just as with 2D ConvNets, this is also used for reducing the length of the 1D input (technically known as subsampling).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "import os\n",
    "import pickle as pk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dir = 'C:/Ankit/Python/Sentiment Analysis/aclImdb/aclImdb'\n",
    "tokenizer_path = 'C:/Ankit/Python/Sentiment Analysis/Tokenizer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are using multiple datasets to train the Sentiment Analysis Classifier.\n",
    "1. IMDB dataset taken from Kaggle <a href=\"https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\" target=\"_blank\">Kaggle IMDB Dataset</a>\n",
    "\n",
    "2. Yelp reviews dataset\n",
    "\n",
    "3. Amazon product reviews\n",
    "\n",
    "4. Random list of positive comments\n",
    "\n",
    "5. Random list of negative comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "#importing IMDB dataset\n",
    "imdb_data=pd.read_csv('C:/Ankit/Python/Sentiment Analysis/IMDB Dataset.csv/IMDB Dataset.csv')\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(10)\n",
    "\n",
    "labels = list()\n",
    "texts = list()\n",
    "for index, row in imdb_data.iterrows():\n",
    "            texts.append(row[0])\n",
    "            if row[1] == 'negative':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "\n",
    "\n",
    "\n",
    "#importing amazon reviews\n",
    "with open('C:/Ankit/Python/Sentiment Analysis/amazon.txt', 'r') as in_file:\n",
    "    stripped = (line.strip() for line in in_file)\n",
    "    lines = (line.split('\\t') for line in stripped if line)\n",
    "    \n",
    "    for row in lines:\n",
    "            texts.append(row[0])\n",
    "            if row[1] == '0':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "\n",
    "#importing yelp reviews\n",
    "with open('C:/Ankit/Python/Sentiment Analysis/yelp.txt', 'r') as in_file:\n",
    "    stripped = (line.strip() for line in in_file)\n",
    "    lines = (line.split('\\t') for line in stripped if line)\n",
    "    \n",
    "    for row in lines:\n",
    "            texts.append(row[0])\n",
    "            if row[1] == '0':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "\n",
    "#importing positive reviews reviews\n",
    "with open('C:/Ankit/Python/Sentiment Analysis/positive_comments.txt', 'r') as in_file:\n",
    "    stripped = (line.strip() for line in in_file)\n",
    "    lines = (line.split('\\t') for line in stripped if line)\n",
    "    \n",
    "    for row in lines:\n",
    "            texts.append(row[0])\n",
    "            labels.append(1)\n",
    "\n",
    "#importing positive reviews reviews\n",
    "with open('C:/Ankit/Python/Sentiment Analysis/negative_comments.txt', 'r') as in_file:\n",
    "    stripped = (line.strip() for line in in_file)\n",
    "    lines = (line.split('\\t') for line in stripped if line)\n",
    "    \n",
    "    for row in lines:\n",
    "            texts.append(row[0])\n",
    "            labels.append(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenizing the data\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 126268 unique tokens. \n",
      "Data Shape: (62662, 500)\n",
      "Shape of data tensor:  (62662, 500)\n",
      "Shape of label tensor:  (62662,)\n"
     ]
    }
   ],
   "source": [
    "# cut off reviews after 500 words\n",
    "max_len = 500 \n",
    "# train on 10000 samples\n",
    "training_samples = 20000\n",
    " # validate on 10000 samples \n",
    "validation_samples = 10000\n",
    "# consider only the top 10000 words\n",
    "max_words = 10000 \n",
    "\n",
    "# import tokenizer with the consideration for only the top 500 words\n",
    "tokenizer = Tokenizer(num_words=max_words) \n",
    "# fit the tokenizer on the texts\n",
    "tokenizer.fit_on_texts(texts) \n",
    "# convert the texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts) \n",
    "\n",
    "# save the tokenizer\n",
    "with open(os.path.join(tokenizer_path, 'tokenizer_m1.pickle'), 'wb') as handle:\n",
    "    pk.dump(tokenizer, handle, protocol=pk.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens. ' % len(word_index))\n",
    "\n",
    " # pad the sequence to the required length to ensure uniformity\n",
    "data = pad_sequences(sequences, maxlen=max_len)\n",
    "print('Data Shape: {}'.format(data.shape))\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print(\"Shape of data tensor: \", data.shape)\n",
    "print(\"Shape of label tensor: \", labels.shape)\n",
    "\n",
    "# split the data into training and validation set but before that shuffle it first\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples:training_samples + validation_samples]\n",
    "y_val = labels[training_samples:training_samples + validation_samples]\n",
    "\n",
    "# test_data\n",
    "x_test = data[training_samples+validation_samples:]\n",
    "y_test = labels[training_samples+validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(tokenizer_path, 'tokenizer_m1.pickle'), 'wb') as handle:\n",
    "    pk.dump(tokenizer, handle, protocol=pk.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # decode the words\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join([reverse_word_index.get(i, '?') for i in sequences[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras import Model, layers\n",
    "from keras import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback Functions\n",
    "\n",
    "We tend to lose control over how our model trains on the provided dataset the moment we call the fit() or fit_generator() method on our model and this means that with a model not “smart” enough, we can only watch it perform very badly during training or quit the training and start allover again. This process can really be expensive and ineffective therefore in order to avoid that, we would like to develop a model that can self-introspect and dynamically take action that will positively affect training. There are many things one cannot predict during training. For instance, one cannot tell the exact number of epochs that will be needed to achieve an optimal validation loss and accuracy.\n",
    "\n",
    "Mostly during training, we tend to use an arbitrary number of epochs and if the model overfits before that number of epochs is reached then we reduce the number of epochs and train again otherwise, we increase the number of epochs and this approach is very wasteful. A much better way to handle this during training is to stop training when we realize that the validation loss is no longer improving. This can be achieved using a Keras callback. A callback is an object (a class instance implementing specific methods) that is passed to the model in the call to fit and that is called by the model at various points during training. It has access to all the available data about the state of the model and its performance, and it can take action: interrupt training, save a model, load a different weight set, or otherwise alter the state of the model.\n",
    "Some ways by which callbacks can be used are:\n",
    "\n",
    "* Model checkpointing — Saving the current weights of the model at different points during training.\n",
    "* Early stopping — Interrupting training when the validation loss is no longer improving (and save the best model obtained during training).\n",
    "* Dynamically adjusting the value of certain parameters during training such as the learning rate optimizer.\n",
    "* Logging training and validation metrics during training or visualizing representations learned by the model as they’re updated. (The Keras progress bar we always see in our terminal during training!)\n",
    "\n",
    "The code below shows the callback functions we have used for Keras -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\callbacks\\tensorboard_v2.py:102: UserWarning: The TensorBoard callback does not support embeddings display when using TensorFlow 2.0. Embeddings-related arguments are ignored.\n",
      "  warnings.warn('The TensorBoard callback does not support '\n"
     ]
    }
   ],
   "source": [
    "# Sample Call-back code\n",
    "callback_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        patience=1,\n",
    "        monitor='acc',\n",
    "    ),\n",
    "    \n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='C:/Ankit/Python/Sentiment Analysis/model/log_dir_m1',\n",
    "        histogram_freq=1,\n",
    "        embeddings_freq=1,\n",
    "    ),\n",
    "\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        filepath='C:/Ankit/Python/Sentiment Analysis/model/movie_sentiment_m1.h5',\n",
    "    ),\n",
    "\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        patience=1,\n",
    "        factor=0.1,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks are passed to the during via the callback argument in the fit() method which takes a list of callbacks. Any number of callbacks can be passed to it.\n",
    "\n",
    "The monitor argument in the EarlyStopping callback monitor’s the model’s validation accuracy and the patience argument interrupts training when the parameter passed to the monitor argument stops improving for more than the number (of epochs) passed to it (in this case 1).\n",
    "\n",
    "The filepath argument in the ModelCheckpoint callback saves the current weights after every epoch to the destination model file and the monitor and save_best_only arguments mean we won’t override the model file unless the validation loss (val_loss) has improved. This allows us to keep the best model seen during training.\n",
    "\n",
    "Also, the ReduceLROnPlateau callback is used to reduce the learning rate when the validation loss has stopped improving. This has proven to be a very effective strategy to get out of local minima during training. The factor argument takes as input a float which is used to divide the learning rate when triggered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 5-layered 1D ConvNet which is flattened at the end using the GlobalMaxPooling1D layer and fed to a Dense layer. Alternatively, the Flatten layer can also be used to accomplish this task. We then make our prediction by feeding the vector obtained from the Dense layer to another Dense layer of 1 unit and a sigmoid activation function. Our choice for a sigmoid activation function at the output layer is because our classification task involves only two classes (either positive or negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 500, 50)           500000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 498, 256)          38656     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 166, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 164, 256)          196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 54, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 52, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 17, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 15, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 3, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,392,161\n",
      "Trainable params: 1,392,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model developing\n",
    "text_input_layer = Input(shape=(500,))\n",
    "embedding_layer = Embedding(max_words, 50)(text_input_layer)\n",
    "text_layer = Conv1D(256, 3, activation='relu')(embedding_layer)\n",
    "text_layer = MaxPooling1D(3)(text_layer)\n",
    "text_layer = Conv1D(256, 3, activation='relu')(text_layer)\n",
    "text_layer = MaxPooling1D(3)(text_layer)\n",
    "text_layer = Conv1D(256, 3, activation='relu')(text_layer)\n",
    "text_layer = MaxPooling1D(3)(text_layer)\n",
    "text_layer = Conv1D(256, 3, activation='relu')(text_layer)\n",
    "text_layer = MaxPooling1D(3)(text_layer)\n",
    "text_layer = Conv1D(256, 3, activation='relu')(text_layer)\n",
    "text_layer = MaxPooling1D(3)(text_layer)\n",
    "text_layer = GlobalMaxPooling1D()(text_layer)\n",
    "text_layer = Dense(256, activation='relu')(text_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(text_layer)\n",
    "model = Model(text_input_layer, output_layer)\n",
    "model.summary()\n",
    "model.compile(optimizer=RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "20000/20000 [==============================] - 155s 8ms/step - loss: 0.6489 - acc: 0.5814 - val_loss: 0.5650 - val_acc: 0.7255\n",
      "Epoch 2/50\n",
      "20000/20000 [==============================] - 158s 8ms/step - loss: 0.4428 - acc: 0.7785 - val_loss: 0.4619 - val_acc: 0.7550\n",
      "Epoch 3/50\n",
      "20000/20000 [==============================] - 156s 8ms/step - loss: 0.3641 - acc: 0.8183 - val_loss: 0.3910 - val_acc: 0.7944\n",
      "Epoch 4/50\n",
      "20000/20000 [==============================] - 155s 8ms/step - loss: 0.3105 - acc: 0.8443 - val_loss: 0.3945 - val_acc: 0.8021\n",
      "Epoch 5/50\n",
      "20000/20000 [==============================] - 161s 8ms/step - loss: 0.2235 - acc: 0.8841 - val_loss: 0.4386 - val_acc: 0.8022\n",
      "Epoch 6/50\n",
      "20000/20000 [==============================] - 157s 8ms/step - loss: 0.2115 - acc: 0.8882 - val_loss: 0.4375 - val_acc: 0.8021\n",
      "Epoch 7/50\n",
      "20000/20000 [==============================] - 163s 8ms/step - loss: 0.2100 - acc: 0.8892 - val_loss: 0.4381 - val_acc: 0.8023\n",
      "Epoch 8/50\n",
      "20000/20000 [==============================] - 162s 8ms/step - loss: 0.2099 - acc: 0.8893 - val_loss: 0.4381 - val_acc: 0.8023\n",
      "Epoch 9/50\n",
      "20000/20000 [==============================] - 168s 8ms/step - loss: 0.2098 - acc: 0.8893 - val_loss: 0.4382 - val_acc: 0.8023\n",
      "Epoch 10/50\n",
      "20000/20000 [==============================] - 160s 8ms/step - loss: 0.2098 - acc: 0.8893 - val_loss: 0.4382 - val_acc: 0.8023\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=50, batch_size=128, callbacks=callback_list,validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os, pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankitagarwal5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = 'C:/Ankit/Python/Sentiment Analysis/Tokenizer'\n",
    "model_path = 'C:/Ankit/Python/Sentiment Analysis/model'\n",
    "model_file = os.path.join(model_path, 'movie_sentiment_m1.h5')\n",
    "tokenizer_file = os.path.join(tokenizer_path, 'tokenizer_m1.pickle')\n",
    "model = load_model(model_file)\n",
    "\n",
    "# load tokenizer\n",
    "with open(tokenizer_file, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_rating(score, decoded_review):\n",
    "    if float(score) >= 0.9:\n",
    "        print('Review: {}\\nSentiment: Strongly Positive\\nScore: {}'.format(decoded_review, score))\n",
    "    elif float(score) >= 0.7 and float(score) < 0.9:\n",
    "        print('Review: {}\\nSentiment: Positive\\nScore: {}'.format(decoded_review, score))\n",
    "    elif float(score) >= 0.5 and float(score) < 0.7:\n",
    "        print('Review: {}\\nSentiment: Okay\\nScore: {}'.format(decoded_review, score))\n",
    "    else:\n",
    "        print('Review: {}\\nSentiment: Negative\\nScore: {}'.format(decoded_review, score))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text_list):\n",
    "    word_index = tokenizer.word_index\n",
    "    sequences = tokenizer.texts_to_sequences(text_list)\n",
    "    data = pad_sequences(sequences, maxlen=500)\n",
    "\n",
    "    # decode the words\n",
    "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "    decoded_review = ' '.join([reverse_word_index.get(i, '?') for i in sequences[0]])\n",
    "    return decoded_review, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_review(source=None, file_type=None):\n",
    "    '''\n",
    "    source: the text, as either a string or a list of strings\n",
    "    file_type: (str): indicating whether we expecting a file containing the\n",
    "    text data or a directory containing a list files holding the text\n",
    "    options: 'file' or 'dir'\n",
    "    '''\n",
    "    text_list = list()\n",
    "    if isinstance(source, str) and file_type is None:\n",
    "        text_list.append(source)\n",
    "        decoded_review, data = decode_review(text_list)\n",
    "        # make prediction\n",
    "        score = model.predict(data)[0][0]\n",
    "        review_rating(score, decoded_review)\n",
    "    \n",
    "    if isinstance(source, list) and file_type is None:\n",
    "        for item in source:\n",
    "            text_list = list()\n",
    "            text_list.append(item)\n",
    "            decoded_review, data = decode_review(text_list)\n",
    "            score = model.predict(data)[0][0]\n",
    "            review_rating(score, decoded_review)\n",
    "    \n",
    "    if isinstance(source, str) and file_type == 'file':\n",
    "        file_data = open(source).read()\n",
    "        text_list.append(file_data)\n",
    "        decoded_review, data = decode_review(text_list)\n",
    "        # make prediction\n",
    "        score = model.predict(data)[0][0]\n",
    "        review_rating(score, decoded_review)\n",
    "    \n",
    "    if isinstance(source, str) and file_type == 'dir':\n",
    "        file_content_holder = list()\n",
    "        for fname in os.listdir(source):\n",
    "            if fname[-4:] == '.txt':\n",
    "                f = open(os.path.join(source, fname),encoding='utf-8')\n",
    "                file_content_holder.append(f.read())\n",
    "                f.close()\n",
    "        for item in file_content_holder:\n",
    "            text_list = list()\n",
    "            text_list.append(item)\n",
    "            decoded_review, data = decode_review(text_list)\n",
    "            score = model.predict(data)[0][0]\n",
    "            review_rating(score, decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history.get('acc')\n",
    "val_acc = history.history.get('val_acc')\n",
    "loss = history.history.get('loss')\n",
    "val_loss = history.history.get('val_loss')\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training Acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#imdb_dir = 'C:/Ankit/Python/Sentiment Analysis/aclImdb/aclImdb'\n",
    "score_review('C:/Ankit/Python/Sentiment Analysis/aclImdb/aclImdb/test/pos', file_type='dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: it was awesome\n",
      "Sentiment: Okay\n",
      "Score: 0.5106653571128845\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_review('It was awesome')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use discrete set of .TXT files that are housed in a directory * pos * and each of those txt files will be scanned and its sentiment will be predicted by model. This has been done to show how to read each file at a time and use of prediction. Each TXT file is a IMDB movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'C:/Ankit/Python/Sentiment Analysis/aclImdb/aclImdb/test/pos'\n",
    "file_type='dir'\n",
    "print(list() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in os.listdir(source):\n",
    "            print(fname)\n",
    "            print(fname[-4:])\n",
    "            if fname[-4:] == '.txt':\n",
    "                f = open(os.path.join(source, fname))\n",
    "                print(f)\n",
    "                file_content_holder.append(f.read())\n",
    "                print(file_content_holder)\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "\n",
    "The key purpose of TensorBoard is to help us visually monitor everything that goes on inside our model during training. Tensorboard gives us access to several relevant features such as -\n",
    "\n",
    "1. visually monitoring metrics during training\n",
    "2. visualizing the architecture of our model\n",
    "3. visualizing histograms of activations and gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using tensorboard, we will need to first create a directory where the log files it generates will be stored using the following command in Jupyter notebook .\n",
    "( Alternatively you may also execute this command in Python Command line $ mkdir log_dir_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir 'C:/Ankit/Python/Sentiment Analysis/model/log_dir_m1'\n",
    "\n",
    "#With the server started, we can then browse to http://localhost:6006 and look at the model training. In addition to the training and validation metrics,\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
