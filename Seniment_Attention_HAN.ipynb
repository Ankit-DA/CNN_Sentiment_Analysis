{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ankitagarwal5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ankitagarwal5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ankitagarwal5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "import math\n",
    "import string\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "import urllib\n",
    "import gzip\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pip\n",
    "#import theano\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer,  text_to_word_sequence\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.utils import plot_model, np_utils\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing IMDB dataset\n",
    "imdb_data=pd.read_csv('C:/Ankit/Python/Sentiment Analysis/IMDB Dataset.csv/IMDB Dataset.csv')\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data=imdb_data,columns=['review',\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  overall\n",
       "0  One of the other reviewers has mentioned that ...      1.0\n",
       "1  A wonderful little production. <br /><br />The...      1.0\n",
       "2  I thought this was a wonderful way to spend ti...      1.0\n",
       "3  Basically there's a family where a little boy ...      0.0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...      1.0\n",
       "5  Probably my all-time favorite movie, a story o...      1.0\n",
       "6  I sure would like to see a resurrection of a u...      1.0\n",
       "7  This show was an amazing, fresh & innovative i...      0.0\n",
       "8  Encouraged by the positive comments about this...      0.0\n",
       "9  If you like original gut wrenching laughter yo...      1.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data.loc[:,'sentiment'] = pd.Categorical(imdb_data.sentiment)\n",
    "categoryToCode = dict( enumerate(imdb_data['sentiment'].cat.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'negative', 1: 'positive'}\n"
     ]
    }
   ],
   "source": [
    "print(categoryToCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Daniel Day-Lewis is the most versatile actor a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>My guess would be this was originally going to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Well, I like to watch bad horror B-Movies, cau...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>This IS the worst movie I have ever seen, as w...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>I have been a Mario fan for as long as I can r...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review sentiment  overall\n",
       "0   One of the other reviewers has mentioned that ...  positive      1.0\n",
       "1   A wonderful little production. <br /><br />The...  positive      1.0\n",
       "2   I thought this was a wonderful way to spend ti...  positive      1.0\n",
       "3   Basically there's a family where a little boy ...  negative      0.0\n",
       "4   Petter Mattei's \"Love in the Time of Money\" is...  positive      1.0\n",
       "..                                                ...       ...      ...\n",
       "95  Daniel Day-Lewis is the most versatile actor a...  positive      1.0\n",
       "96  My guess would be this was originally going to...  negative      0.0\n",
       "97  Well, I like to watch bad horror B-Movies, cau...  negative      0.0\n",
       "98  This IS the worst movie I have ever seen, as w...  negative      0.0\n",
       "99  I have been a Mario fan for as long as I can r...  positive      1.0\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_df\n",
    "df.loc[df['sentiment'] == 'positive', 'overall'] = 1\n",
    "df.loc[df['sentiment'] == 'negative', 'overall'] = 0\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['sentiment']\n",
    "df.columns = ['Text','overall']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  overall\n",
       "0  One of the other reviewers has mentioned that ...      1.0\n",
       "1  A wonderful little production. <br /><br />The...      1.0\n",
       "2  I thought this was a wonderful way to spend ti...      1.0\n",
       "3  Basically there's a family where a little boy ...      0.0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...      1.0\n",
       "5  Probably my all-time favorite movie, a story o...      1.0\n",
       "6  I sure would like to see a resurrection of a u...      1.0\n",
       "7  This show was an amazing, fresh & innovative i...      0.0\n",
       "8  Encouraged by the positive comments about this...      0.0\n",
       "9  If you like original gut wrenching laughter yo...      1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review=df\n",
    "df_review.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean String\n",
    "def cleanString(review,stopWords):\n",
    "    \"\"\"\n",
    "    Cleans input string using set rules.\n",
    "    Cleaning rules:         Every word is lemmatized and lowercased. Stopwords and non alpha-numeric words are removed.\n",
    "                            Each sentence ends with a period.\n",
    "    Input:   review       - string(in sentence structure)\n",
    "             stopWords    - set of strings which should be removed from review\n",
    "    Output:  returnString - cleaned input string\n",
    "             idx_list     - list of lists, one list is equal to one sentence. In every list are the index\n",
    "                            of each word as they appeared in the non cleaned sentence\n",
    "                            e.g. nonCleaned = \"This is a test.\" -> cleaned = \"This test.\" -> cleaned_index = [[0,3]]\n",
    "    \"\"\"\n",
    "    # Init the Wordnet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    returnString = \"\"\n",
    "    sentence_token = tokenize.sent_tokenize(review)\n",
    "    idx_list = []\n",
    "    for j in range(len(sentence_token)):\n",
    "        single_sentence = tokenize.word_tokenize(sentence_token[j])\n",
    "        sentences_filtered = [(idx,lemmatizer.lemmatize(w.lower())) for idx,w in enumerate(single_sentence) \n",
    "                              if w.lower() not in stopWords and w.isalnum()]\n",
    "        idx_list.append([x[0] for x in sentences_filtered])\n",
    "        word_list = [x[1] for x in sentences_filtered]\n",
    "        returnString = returnString + ' '.join(word_list) + ' . '\n",
    "    \n",
    "    return returnString, idx_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 of 50000 articles cleaned.\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewer mentioned watching 1 oz episode h...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production . br br filming te...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  overall\n",
       "0  one reviewer mentioned watching 1 oz episode h...      1.0\n",
       "1  wonderful little production . br br filming te...      1.0\n",
       "2  thought wonderful way spend time hot summer we...      1.0\n",
       "3  basically family little boy jake think zombie ...      0.0\n",
       "4  petter mattei love time money visually stunnin...      1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cleans raw data using the cleanString() function from above.\n",
    "English stopwords are used from nltk library.\n",
    "Cleaned dataset is saved in 'data_cleaned' pandas dataframe.\n",
    "Labels are converted to numbers,\n",
    "\"\"\"\n",
    "articles = []\n",
    "n = data_df['Text'].shape[0]\n",
    "col_number = data_df.columns.get_loc('Text')\n",
    "stopWords = set(stopwords.words('english'))\n",
    "data_cleaned = data_df.copy()\n",
    "for i in range(n):\n",
    "    temp_string,idx_string = cleanString(data_df.iloc[i,col_number],stopWords)\n",
    "    articles.append(temp_string)\n",
    "    print(str(i+1)+' of '+str(n)+\" articles cleaned.\",end='\\r')\n",
    "    \n",
    "data_cleaned.loc[:,'Text'] = pd.Series(articles,index=data_df.index)\n",
    "#data_cleaned.loc[:,'Category'] = pd.Categorical(data_cleaned.Category)\n",
    "#data_cleaned['Code'] = data_cleaned.Category.cat.codes\n",
    "#categoryToCode = dict( enumerate(data_cleaned['Category'].cat.categories))\n",
    "\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 200000 # maximum number of unique words that should be included in the tokenized word index\n",
    "MAX_SENTENCE_NUM = 40 # maximum number of sentences in one document\n",
    "MAX_WORD_NUM = 50     # maximum number of words in each sentence\n",
    "EMBED_SIZE = 100      # vector size of word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 89084\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using the keras Tokenizer class a word index is built.\n",
    "The most 'MAX_FEATURES' used words are tokenized to a number.\n",
    "this dictionary is saved in word_index\n",
    "\"\"\"\n",
    "texts = []\n",
    "n = data_cleaned['Text'].shape[0]\n",
    "for i in range(n):\n",
    "    s = data_cleaned['Text'].iloc[i]\n",
    "    s = ' '.join([word.strip(string.punctuation) for word in s.split() if word.strip(string.punctuation) is not \"\"])\n",
    "    texts.append(s)\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES,lower=True, oov_token=None)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Number of tokens: ' + str(len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewer mentioned watching 1 oz episode h...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production . br br filming te...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  overall\n",
       "0  one reviewer mentioned watching 1 oz episode h...      1.0\n",
       "1  wonderful little production . br br filming te...      1.0\n",
       "2  thought wonderful way spend time hot summer we...      1.0\n",
       "3  basically family little boy jake think zombie ...      0.0\n",
       "4  petter mattei love time money visually stunnin...      1.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Total absent words are 2661 which is 2.99 % of total words\n",
      "Words with 2 or less mentions 45076 which is 50.60 % of total words\n",
      "41347 words to proceed.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A pre-trained word to vector is used from GloVe by Pennington et. al.\n",
    "Source: https://nlp.stanford.edu/projects/glove/\n",
    "The data was trained on wikipedia articles. Each word is described by a 100d vector.\n",
    "\"\"\"\n",
    "\n",
    "# Load word vectors from pre-trained dataset\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('C:/Ankit/Python/Sentiment Analysis/glove.6B', 'glove.6B.100d.txt'), encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# Embedding\n",
    "\n",
    "EMBED_SIZE = 100\n",
    "\n",
    "min_wordCount = 2\n",
    "absent_words = 0\n",
    "small_words = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "word_counts = tokenizer.word_counts\n",
    "for word, i in word_index.items():\n",
    "    if word_counts[word] > min_wordCount:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            absent_words += 1\n",
    "    else:\n",
    "        small_words += 1\n",
    "print('Total absent words are', absent_words, 'which is', \"%0.2f\" % (absent_words * 100 / len(word_index)),\n",
    "      '% of total words')\n",
    "print('Words with '+str(min_wordCount)+' or less mentions', small_words, 'which is', \"%0.2f\" % (small_words * 100 / len(word_index)),\n",
    "      '% of total words')\n",
    "print(str(len(word_index)-small_words-absent_words) + ' words to proceed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.013786  ,  0.38216001,  0.53236002,  0.15261   , -0.29694   ,\n",
       "       -0.20558   , -0.41846001, -0.58437002, -0.77354997, -0.87866002,\n",
       "       -0.37858   , -0.18516   , -0.12800001, -0.20584001, -0.22925   ,\n",
       "       -0.42598999,  0.3725    ,  0.26076999, -1.07019997,  0.62915999,\n",
       "       -0.091469  ,  0.70348001, -0.4973    , -0.77691001,  0.66044998,\n",
       "        0.09465   , -0.44893   ,  0.018917  ,  0.33146   , -0.35021999,\n",
       "       -0.35789001,  0.030313  ,  0.22253001, -0.23236001, -0.19719   ,\n",
       "       -0.0053125 , -0.25848001,  0.58081001, -0.10705   , -0.17845   ,\n",
       "       -0.16205999,  0.087086  ,  0.63028997, -0.76648998,  0.51618999,\n",
       "        0.14072999,  1.01900005, -0.43136001,  0.46138   , -0.43584999,\n",
       "       -0.47567999,  0.19226   ,  0.36065   ,  0.78987002,  0.088945  ,\n",
       "       -2.78139997, -0.15366   ,  0.01015   ,  1.17980003,  0.15167999,\n",
       "       -0.050112  ,  1.26259995, -0.77526999,  0.36030999,  0.95761001,\n",
       "       -0.11385   ,  0.28035   , -0.02591   ,  0.31246001, -0.15424   ,\n",
       "        0.37779999, -0.13598999,  0.29460001, -0.31579   ,  0.42943001,\n",
       "        0.086969  ,  0.019169  , -0.27241999, -0.31696001,  0.37327   ,\n",
       "        0.61997002,  0.13889   ,  0.17188001,  0.30362999, -1.27760005,\n",
       "        0.044423  , -0.52736002, -0.88536   , -0.19428   , -0.61947   ,\n",
       "       -0.10146   , -0.26301   , -0.061707  ,  0.36627001, -0.95222998,\n",
       "       -0.39346001, -0.69182998, -1.04260004,  0.28854999,  0.63055998])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embedding_matrix[word_index['great']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  50000\n",
      "Training:  40000 , Percentage:  0.8\n",
      "Validation:  5000 , Percentage:  0.1\n",
      "Test: 5000 , Percentage:  0.1\n"
     ]
    }
   ],
   "source": [
    "def split_df(dataframe, column_name, training_split, validation_split, test_split):\n",
    "    \"\"\"\n",
    "    Splits a pandas dataframe into trainingset, validationset and testset in specified ratio.\n",
    "    All sets are balanced, which means they have the same ratio for each category as the full set.\n",
    "    Input:   dataframe        - Pandas Dataframe, should include a column for data and one for categories\n",
    "             column_name      - Name of dataframe column which contains the categorical output values\n",
    "             training_split   - from ]0,1[, default = 0.6\n",
    "             validation_split - from ]0,1[, default = 0.2        \n",
    "             test_split       - from ]0,1[, default = 0.2\n",
    "                                Sum of all splits need to be 1\n",
    "    Output:  train            - Pandas DataFrame of trainset\n",
    "             validation       - Pandas DataFrame of validationset\n",
    "             test             - Pandas DataFrame of testset\n",
    "    \"\"\"\n",
    "    if training_split + validation_split + test_split != 1.0:\n",
    "        raise ValueError('Split paramter sum should be 1.0')\n",
    "        \n",
    "    total = len(dataframe.index)\n",
    " \n",
    "    train = dataframe.reset_index().groupby(column_name).apply(lambda x: x.sample(frac=training_split))\\\n",
    "    .reset_index(drop=True).set_index('index')\n",
    "    train = train.sample(frac=1)\n",
    "    temp_df = dataframe.drop(train.index)\n",
    "    validation = temp_df.reset_index().groupby(column_name)\\\n",
    "    .apply(lambda x: x.sample(frac=validation_split/(test_split+validation_split)))\\\n",
    "           .reset_index(drop=True).set_index('index')\n",
    "    validation = validation.sample(frac=1)\n",
    "    test = temp_df.drop(validation.index)\n",
    "    test = test.sample(frac=1)\n",
    "    \n",
    "    print('Total: ', len(dataframe))\n",
    "    print('Training: ', len(train), ', Percentage: ', len(train)/len(dataframe))\n",
    "    print('Validation: ', len(validation), ', Percentage: ', len(validation)/len(dataframe))\n",
    "    print('Test:', len(test), ', Percentage: ', len(test)/len(dataframe))\n",
    "\n",
    "    return train, validation, test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    Hierarchial Attention Layer as described by Hierarchical Attention Networks for Document Classification(2016)\n",
    "    - Yang et. al.\n",
    "    Source: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
    "    Theano backend\n",
    "    \"\"\"\n",
    "    def __init__(self,attention_dim=100,return_coefficients=False,**kwargs):\n",
    "        # Initializer \n",
    "        self.supports_masking = True\n",
    "        self.return_coefficients = return_coefficients\n",
    "        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Builds all weights\n",
    "        # W = Weight matrix, b = bias vector, u = context vector\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)),name='W')\n",
    "        self.b = K.variable(self.init((self.attention_dim, )),name='b')\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, hit, mask=None):\n",
    "        # Here, the actual calculation is done\n",
    "        uit = K.bias_add(K.dot(hit, self.W),self.b)\n",
    "        uit = K.tanh(uit)\n",
    "        \n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "        \n",
    "        if mask is not None:\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = hit * ait\n",
    "        \n",
    "        if self.return_coefficients:\n",
    "            return [K.sum(weighted_input, axis=1), ait]\n",
    "        else:\n",
    "            return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_coefficients:\n",
    "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'word_attention_1/W:0' shape=(100, 100) dtype=float32> W\n",
      "tracking <tf.Variable 'word_attention_1/b:0' shape=(100,) dtype=float32> b\n",
      "tracking <tf.Variable 'word_attention_1/u:0' shape=(100, 1) dtype=float32> u\n",
      "tracking <tf.Variable 'sent_attention_1/W:0' shape=(100, 100) dtype=float32> W\n",
      "tracking <tf.Variable 'sent_attention_1/b:0' shape=(100,) dtype=float32> b\n",
      "tracking <tf.Variable 'sent_attention_1/u:0' shape=(100, 1) dtype=float32> u\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_input (InputLayer)      (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "word_embedding (Embedding)   (None, 50, 100)           8908500   \n",
      "_________________________________________________________________\n",
      "word_gru (Bidirectional)     (None, 50, 100)           45300     \n",
      "_________________________________________________________________\n",
      "word_dense (Dense)           (None, 50, 100)           10100     \n",
      "_________________________________________________________________\n",
      "word_attention (AttentionLay [(None, 100), (None, 100, 10200     \n",
      "=================================================================\n",
      "Total params: 8,974,100\n",
      "Trainable params: 65,600\n",
      "Non-trainable params: 8,908,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sent_input (InputLayer)      (None, 40, 50)            0         \n",
      "_________________________________________________________________\n",
      "sent_linking (TimeDistribute (None, 40, 100)           8974100   \n",
      "_________________________________________________________________\n",
      "sent_gru (Bidirectional)     (None, 40, 100)           45300     \n",
      "_________________________________________________________________\n",
      "sent_dense (Dense)           (None, 40, 100)           10100     \n",
      "_________________________________________________________________\n",
      "sent_attention (AttentionLay [(None, 100), (None, 100, 10200     \n",
      "_________________________________________________________________\n",
      "sent_dropout (Dropout)       (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 9,039,902\n",
      "Trainable params: 131,402\n",
      "Non-trainable params: 8,908,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create Keras functional model for hierarchical attention network\n",
    "\"\"\"\n",
    "embedding_layer = Embedding(len(word_index) + 1,EMBED_SIZE,weights=[embedding_matrix], \n",
    "                            input_length=MAX_WORD_NUM, trainable=False,name='word_embedding')\n",
    "\n",
    "# Words level attention model\n",
    "word_input = Input(shape=(MAX_WORD_NUM,), dtype='int32',name='word_input')\n",
    "word_sequences = embedding_layer(word_input)\n",
    "word_gru = Bidirectional(GRU(50, return_sequences=True),name='word_gru')(word_sequences)\n",
    "word_dense = Dense(100, activation='relu', name='word_dense')(word_gru) \n",
    "word_att,word_coeffs = AttentionLayer(EMBED_SIZE,True,name='word_attention')(word_dense)\n",
    "wordEncoder = Model(inputs = word_input,outputs = word_att)\n",
    "\n",
    "# Sentence level attention model\n",
    "sent_input = Input(shape=(MAX_SENTENCE_NUM,MAX_WORD_NUM), dtype='int32',name='sent_input')\n",
    "sent_encoder = TimeDistributed(wordEncoder,name='sent_linking')(sent_input)\n",
    "sent_gru = Bidirectional(GRU(50, return_sequences=True),name='sent_gru')(sent_encoder)\n",
    "sent_dense = Dense(100, activation='relu', name='sent_dense')(sent_gru) \n",
    "sent_att,sent_coeffs = AttentionLayer(EMBED_SIZE,return_coefficients=True,name='sent_attention')(sent_dense)\n",
    "sent_drop = Dropout(0.5,name='sent_dropout')(sent_att)\n",
    "preds = Dense(2, activation='softmax',name='output')(sent_drop)\n",
    "\n",
    "# Model compile\n",
    "model = Model(sent_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "print(wordEncoder.summary())\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(series,class_dict):\n",
    "    \"\"\"\n",
    "    Converts category labels to vectors,\n",
    "    Input:     series     - pandas Series containing numbered category labels\n",
    "               class_dict - dictionary of integer to category string \n",
    "                            e.g. {0: 'business', 1: 'entertainment', 2: 'politics', 3: 'sport', 4: 'tech'}\n",
    "    Output:    Array      - numpy array containing categories converted to lists\n",
    "                            e.g. 0:'business'      -> [1 0 0 0 0]\n",
    "                                 1:'entertainment' -> [0 1 0 0 0]\n",
    "                                 2:'politics'      -> [0 0 1 0 0]\n",
    "                                 3:'sport'         -> [0 0 0 1 0]\n",
    "                                 4:'tech'          -> [0 0 0 0 1]\n",
    "    \"\"\"\n",
    "    n_classes = len(class_dict)\n",
    "    new_dict = {}\n",
    "    for key,value in class_dict.items():\n",
    "        cat_list = [0] * n_classes\n",
    "        cat_list[key] = 1\n",
    "        new_dict[key] = cat_list\n",
    "    y_cat = []\n",
    "    for key,value in series.iteritems():\n",
    "        y_cat.append(new_dict[value])\n",
    "    return np.array(y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordToSeq(text,word_index,max_sentences,max_words,max_features):\n",
    "    \"\"\"\n",
    "    Converts a string to a numpy matrix where each word is tokenized.\n",
    "    Arrays are zero-padded to max_sentences and max_words length.\n",
    "    \n",
    "    Input:    text           - string of sentences\n",
    "              word_index     - trained word_index\n",
    "              max_sentences  - maximum number of sentences allowed per document for HAN\n",
    "              max_words      - maximum number of words in each sentence for HAN\n",
    "              max_features   - maximum number of unique words to be tokenized\n",
    "    Output:   data           - Numpy Matrix of size [max_sentences x max_words]\n",
    "    \"\"\"\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    data = np.zeros((max_sentences, max_words), dtype='int32')\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j< max_sentences:\n",
    "            wordTokens = tokenize.word_tokenize(sent.rstrip('.'))\n",
    "            wordTokens = [w for w in wordTokens]\n",
    "            k=0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                try:\n",
    "                    if k<max_words and word_index[word]<max_features:\n",
    "                        data[j,k] = word_index[word]\n",
    "                        k=k+1\n",
    "                except:\n",
    "                    pass\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  50000\n",
      "Training:  40000 , Percentage:  0.8\n",
      "Validation:  5000 , Percentage:  0.1\n",
      "Test: 5000 , Percentage:  0.1\n"
     ]
    }
   ],
   "source": [
    "train, validation, test = split_df(data_cleaned, 'overall',0.8,0.1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "paras = []\n",
    "for i in range(train['Text'].shape[0]):\n",
    "    sequence = wordToSeq(train['Text'].iloc[i],word_index,MAX_SENTENCE_NUM,MAX_WORD_NUM,MAX_FEATURES)\n",
    "    paras.append(sequence)\n",
    "x_train = np.array(paras)\n",
    "y_train = to_categorical(train['overall'],categoryToCode)\n",
    "\n",
    "#Validation\n",
    "paras = []\n",
    "for i in range(validation['Text'].shape[0]):\n",
    "    sequence = wordToSeq(validation['Text'].iloc[i],word_index,MAX_SENTENCE_NUM,MAX_WORD_NUM,MAX_FEATURES)\n",
    "    paras.append(sequence)\n",
    "x_val = np.array(paras)\n",
    "y_val = to_categorical(validation['overall'],categoryToCode)\n",
    "\n",
    "#Test\n",
    "paras = []\n",
    "for i in range(test['Text'].shape[0]):\n",
    "    sequence = wordToSeq(test['Text'].iloc[i],word_index,MAX_SENTENCE_NUM,MAX_WORD_NUM,MAX_FEATURES)\n",
    "    paras.append(sequence)\n",
    "x_test = np.array(paras)\n",
    "y_test = to_categorical(test['overall'],categoryToCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 5000 samples\n",
      "Epoch 1/7\n",
      "40000/40000 [==============================] - 1681s 42ms/step - loss: 0.3878 - acc: 0.8209 - val_loss: 0.3165 - val_acc: 0.8702\n",
      "Epoch 2/7\n",
      "40000/40000 [==============================] - 1587s 40ms/step - loss: 0.3024 - acc: 0.8736 - val_loss: 0.3029 - val_acc: 0.8764\n",
      "Epoch 3/7\n",
      "40000/40000 [==============================] - 2199s 55ms/step - loss: 0.2715 - acc: 0.8877 - val_loss: 0.3044 - val_acc: 0.8724\n",
      "Epoch 4/7\n",
      "40000/40000 [==============================] - 1762s 44ms/step - loss: 0.2482 - acc: 0.8977 - val_loss: 0.2781 - val_acc: 0.8888\n",
      "Epoch 5/7\n",
      "40000/40000 [==============================] - 2093s 52ms/step - loss: 0.2226 - acc: 0.9105 - val_loss: 0.2881 - val_acc: 0.8850\n",
      "Epoch 6/7\n",
      "40000/40000 [==============================] - 2284s 57ms/step - loss: 0.1996 - acc: 0.9210 - val_loss: 0.2825 - val_acc: 0.8882\n",
      "Epoch 7/7\n",
      "40000/40000 [==============================] - 1771s 44ms/step - loss: 0.1712 - acc: 0.9343 - val_loss: 0.2957 - val_acc: 0.8854\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=7, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 54s 11ms/step\n",
      "Test set accuracy:  0.8802000284194946\n",
      "Test set loss:  0.3121328710079193\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test,y_test)\n",
    "print(\"Test set accuracy: \",acc)\n",
    "print(\"Test set loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Saves the model in a hdf5 file\n",
    "model.save('C:/Ankit/Python/Sentiment Analysis/Model_7epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 69s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3121328710079193, 0.8802000284194946]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=x_test, y=y_test, batch_size=None, verbose=1, sample_weight=None, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[343  13  38 ...   0   0   0]\n",
      " [  4 781 224 ...   0   0   0]\n",
      " [  3 935  39 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]] [1 0]\n"
     ]
    }
   ],
   "source": [
    "print(x_test[5],y_test[5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
